# -*- coding: utf-8 -*-
"""MLSN_Davis_House_Price_Prediction_Random_Forrest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lxc2YSvcLo4_q2YQP0CPpg7ipysRcTxQ
"""

# Data Processing
'''import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
uploaded = file.upload()


# Modelling
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

df=pd.read.csv('davis_housing_clean_2.csv')'''

# Import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Load your data
from google.colab import files
uploaded = files.upload()

# Read the CSV file
df = pd.read_csv('davis_housing_clean_2.csv')

# Basic information about the dataset
print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())

print("\nData Info:")
print(df.info())

print("\nDescriptive Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Let's examine potential outliers, especially in acre_lot
plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
sns.boxplot(y=df['price'])
plt.title('Price Distribution')

plt.subplot(2, 3, 2)
sns.boxplot(y=df['acre_lot'])
plt.title('Acre Lot Distribution')

plt.subplot(2, 3, 3)
sns.boxplot(y=df['house_size'])
plt.title('House Size Distribution')

plt.tight_layout()
plt.show()

# Handle extreme outliers in acre_lot - let's cap them
df_clean = df.copy()
# Cap acre_lot at 10 acres (reasonable for residential properties)
df_clean['acre_lot'] = np.where(df_clean['acre_lot'] > 10, 10, df_clean['acre_lot'])

print("Acre lot statistics after capping:")
print(df_clean['acre_lot'].describe())

# Separate features (X) and target (y)
X = df_clean.drop('price', axis=1)  # All columns except price
y = df_clean['price']  # What we want to predict

print("Features shape:", X.shape)
print("Target shape:", y.shape)
print("Feature names:", list(X.columns))

# Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Testing set: {X_test.shape[0]} samples")

# Initialize the model with some sensible parameters
rf_model = RandomForestRegressor(
    n_estimators=100,  # Number of trees
    random_state=42,   # For reproducibility
    max_depth=10,      # Prevent overfitting
    n_jobs=-1         # Use all available cores
)

# Train the model
rf_model.fit(X_train, y_train)

print("Model training completed!")

# Make predictions on test set
y_pred = rf_model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("=== MODEL EVALUATION RESULTS ===")
print(f"Mean Absolute Error (MAE): ${mae:,.2f}")
print(f"Root Mean Square Error (RMSE): ${rmse:,.2f}")
print(f"R-squared (R²) Score: {r2:.4f}")
print(f"Average House Price: ${y.mean():,.2f}")

# Interpretation help
print("\n=== WHAT THESE METRICS MEAN ===")
print(f"• MAE: On average, predictions are within ${mae:,.2f} of actual prices")
print(f"• RMSE: ${rmse:,.2f} (penalizes large errors more heavily)")
print(f"• R²: {r2:.1%} of price variation is explained by our features")

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("Feature Importance:")
print(feature_importance)

# Visualize feature importance
plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance, x='importance', y='feature')
plt.title('Random Forest Feature Importance')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.show()


plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted House Prices')
plt.show()

# Residual plot
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Prices')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()